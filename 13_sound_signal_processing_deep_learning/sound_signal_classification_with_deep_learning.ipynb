{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4571ef",
   "metadata": {},
   "source": [
    "# SOUND SIGNAL CLASSIFICATION USING DEEP LEARNING PROJECT\n",
    "\n",
    "This project consists of 3 main steps:\n",
    "\n",
    "- Step 1. We will prepare our dataset for analysis and extract sound signal features from  audio files using Mel-Frequency Cepstral Coefficients(MFCC).\n",
    "- Step 2. Then we will build a Convolutional Neural Networks (CNN) model and train our model with our dataset. \n",
    "- Step 3: Finally We Predict an Audio File's Class Using Our CNN Deep Learning Model\n",
    "\n",
    "\n",
    "\n",
    "We will use UrbanSound8K Dataset, download Link is here: https://urbansounddataset.weebly.com/download-urbansound8k.html\n",
    "\n",
    "Dataset folder and this source code should be on same directory..\n",
    "\n",
    "Don't forget to install librosa library using anaconda promt with the following command line:\n",
    "\n",
    "conda install -c conda-forge librosa\n",
    "\n",
    "<IMG src=\"audiosignal.png\" width=\"500\" height=\"250\">\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0f3c8",
   "metadata": {},
   "source": [
    "### Step 1: We will prepare our dataset for analysis and extract sound signal features from  audio files using Mel-Frequency Cepstral Coefficients(MFCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180a909",
   "metadata": {},
   "source": [
    "Every signal has its own characteristics. In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound. Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.\n",
    "\n",
    "You can get detailed info about MFC on : https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=0s\n",
    "\n",
    "So by using librosa library we will get characteristics of every audio signal in our dataset and hold them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I want to show to how librosa handles sound signals.\n",
    "# Let's read an example audio signal using librosa\n",
    "audio_file_path='UrbanSound8K/17973-2-0-32.wav'\n",
    "\n",
    "librosa_audio_data, librosa_sample_rate = librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important thing you should know about librosa is librosa converts any stereo(2 channel) signal into mono(single channel). \n",
    "# So librosa converted signal data is one dimensional since it converts all signals(2 channels) into single channel(mono) \n",
    "# and get signal characteristics of your sound file over this mono signal form..\n",
    "\n",
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecee3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the librosa audio data\n",
    "# Audio with 1 channel \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(librosa_audio_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d5b9f",
   "metadata": {},
   "source": [
    "#### Here We will Extract Features of all Sound  Signals in the UrbanSound8K Dataset\n",
    "\n",
    "Now we will calculate the Mel-Frequency Cepstral Coefficients(MFCC) of the audio samples. The MFCC calculate the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. Using this audio signal characteristics we can identify audio features for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=45)   #n_mfcc: number of MFCCs to return \n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for extracting MFC coefficients from signals using librosa:\n",
    "\n",
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=45)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to find all the files in directory:\n",
    "def find_files(directory, pattern):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, pattern):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename\n",
    "\n",
    "\n",
    "dataset =[]                \n",
    "                \n",
    "for filename in find_files(\"UrbanSound8K/audio\", \"*.wav\"):    \n",
    "#    print(\"Found wav source:\", filename)\n",
    "    label = filename.split(\".wav\")[0][-5]\n",
    "    if label == '-':\n",
    "        label = filename.split(\".wav\")[0][-6]\n",
    "    dataset.append({\"file_name\" : filename, \"label\" : label})\n",
    "  \n",
    "    \n",
    "    \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7692c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's iterate every sound file and extract features using MFC Coefficients of librosa \n",
    "# using features_extractor method we defined above:\n",
    "extracted_features=[]\n",
    "\n",
    "dataset['data'] = dataset['file_name'].apply(features_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1885a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change column names:\n",
    "dataset = dataset.rename(columns={'label': 'class'})\n",
    "dataset = dataset.rename(columns={'data': 'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d646b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa589e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary column from dataframe..\n",
    "dataset.drop(['file_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will convert extracted_features to Pandas dataframe\n",
    "extracted_features_df = pd.DataFrame(dataset,columns=['class','feature'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f589bdd",
   "metadata": {},
   "source": [
    "### Defining Train and Validation Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f07897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd605ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b77c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should perform Label Encoding since we need one hot encoded values for output classes in our model (1s and 0s)\n",
    "\n",
    "# Please remember one-hot encoding:\n",
    "# 1 0 0 0 0 0 0 0 0 0 => air_conditioner\n",
    "# 0 1 0 0 0 0 0 0 0 0 => car_horn\n",
    "# 0 0 1 0 0 0 0 0 0 0 => children_playing\n",
    "# 0 0 0 1 0 0 0 0 0 0 => dog_bark\n",
    "# ...\n",
    "# 0 0 0 0 0 0 0 0 0 1 => street_music\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split dataset as Train and Test\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb306c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e24947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f90202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041361f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4be45",
   "metadata": {},
   "source": [
    "### Step 2: We will Build a Convolutional Neural Network (CNN) Model and Train Our Model with processed sound signals of UrbanSound8K Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes we have? We should  use it in our model\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6121a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start building our CNN model..\n",
    "\n",
    "model=Sequential()\n",
    "# 1. hidden layer\n",
    "model.add(Dense(125,input_shape=(45,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 2. hidden layer\n",
    "model.add(Dense(250))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 3. hidden layer\n",
    "model.add(Dense(125))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f68f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trianing the model\n",
    "\n",
    "epochscount = 300\n",
    "num_batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=epochscount, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f621887",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_test_set_accuracy = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(validation_test_set_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1834186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c3225",
   "metadata": {},
   "source": [
    "### Step 3: Finally We Predict an Audio File's Class Using Our CNN Deep Learning Model\n",
    "\n",
    "We first preprocess the new audio data and then predict the class.\n",
    "\n",
    "\n",
    "You can download example_sound1_children_playing.wav from the link here: https://www.epidemicsound.com/track/qDBYeKjWF0/\n",
    "\n",
    "You can download example_sound2_siren.wav from the link here: https://www.epidemicsound.com/track/ByOBJyDp8P/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfe778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can download example_sound1_children_playing.wav from the link here: https://www.epidemicsound.com/track/qDBYeKjWF0/\n",
    "# You can download example_sound2_siren.wav from the link here: https://www.epidemicsound.com/track/ByOBJyDp8P/\n",
    "\n",
    "filename=\"UrbanSound8K/example_sound1_children_playing.wav\"\n",
    "sound_signal, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=sound_signal, sr=sample_rate, n_mfcc=45)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09074f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_scaled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ea1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66607a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = model.predict(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_classes = [\"air_conditioner\",\"car_horn\",\"children_playing\",\"dog_bark\",\"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\", \"siren\", \"street_music\"]\n",
    "\n",
    "result = np.argmax(result_array[0])\n",
    "print(result_classes[result]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e673a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9efe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6688bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a8549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
